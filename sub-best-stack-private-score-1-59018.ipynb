{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.004735,
     "end_time": "2020-12-19T20:16:41.586171",
     "exception": false,
     "start_time": "2020-12-19T20:16:41.581436",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Data Science Competition- Social Media Prediction\n",
    "\n",
    "### Team Name - Student Baseline\n",
    "\n",
    "### Team Members:\n",
    "\n",
    "Balaji Balasubramanian\n",
    "\n",
    "Patcharin Cheng \n",
    "\n",
    "Arjun Vaithilingam Sudhakar\n",
    "\n",
    "### Prerequisite:\n",
    "\n",
    "1.  Make sure the dataset is linked under the following directory/load the competition dataset in Kaggle,\n",
    "\n",
    "/kaggle/input/ift6758-a20/...\n",
    "or, The data path file has been written to run it on Kaggle notebooks, rename the path file if required.\n",
    "\n",
    "2. The codes require pandas, numpy, sklearn, xgboost, mlxtend-0.18.0, os, random, and warnings libraries to run.\n",
    "\n",
    "Please update mlxtend to the latest version(0.18.0) before running it.(Kaggle notebook uses mlxtend-0.18.0 by default, hence no need to update here).\n",
    "\n",
    "### Steps to run the program:\n",
    "\n",
    "Step 1: run notebook \"Sub-Best_stack-private-score-1.59018\"\n",
    "\n",
    "Step 2: A csv file will be generated (model prediction on the test data after training)\n",
    "\n",
    "Step 3: Upload the csv file created under Submit Prediction in Kaggle to get the accuracy score to estimate the model generalized performance\n",
    "\n",
    "### This notebook will generate the score as following:\n",
    "\n",
    "Public Score = 1.70724\n",
    "\n",
    "Private Score = 1.59018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "a5e47cda-93c2-4247-8a85-07afb0561fbd",
    "_uuid": "6e0799b4-e6e4-4eb9-b2c8-ccddb29710a3",
    "execution": {
     "iopub.execute_input": "2020-12-19T20:16:41.722732Z",
     "iopub.status.busy": "2020-12-19T20:16:41.621807Z",
     "iopub.status.idle": "2020-12-19T20:17:14.403424Z",
     "shell.execute_reply": "2020-12-19T20:17:14.404127Z"
    },
    "papermill": {
     "duration": 32.814506,
     "end_time": "2020-12-19T20:17:14.404369",
     "exception": false,
     "start_time": "2020-12-19T20:16:41.589863",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Team Name- Student Baseline\n",
    "\n",
    "# Team Members\n",
    "# Balaji Balasubramanian\n",
    "# Patcharin Cheng \n",
    "# Arjun Vaithilingam Sudhakar\n",
    "\n",
    "# This is the code for the model that can reproduce our best score.\n",
    "\n",
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import os\n",
    "import random\n",
    "import warnings\n",
    "from mlxtend.regressor import StackingCVRegressor\n",
    "from xgboost import XGBRegressor\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds (for reproducibility requirement)\n",
    "os.environ['PYTHONHASHSEED']=str(1)\n",
    "np.random.seed(1)\n",
    "random.seed(1)\n",
    "    \n",
    "\n",
    "# Converting the sparse user time zone column into 7 unique categories\n",
    "time_zone_dict = {\n",
    "'Eastern Time (US & Canada)':'USA',\n",
    "'Pacific Time (US & Canada)':'USA',\n",
    "'Central Time (US & Canada)':'USA',\n",
    "'Central Time (US & Canada)':'USA',\n",
    "'London':'Europe',\n",
    "'Brasilia':'Latin America',\n",
    "'Paris':'Europe',\n",
    "'Quito':'Latin America',\n",
    "'Jakarta':'Asia',\n",
    "'Amsterdam':'Europe',\n",
    "'Mexico City':'Europe',\n",
    "'Madrid':'Europe',\n",
    "'New Delhi':'Asia',\n",
    "'Istanbul':'Middle East',\n",
    "'Hawaii':'USA',\n",
    "'Tokyo':'Asia',\n",
    "'Rome':'Europe',\n",
    "'Santiago':'Latin America',\n",
    "'Greenland':'Europe',\n",
    "'Buenos Aires':'Europe',\n",
    "'Mountain Time (US & Canada)':'USA',\n",
    "'Riyadh':'Middle East',\n",
    "'Caracas':'Latin America',\n",
    "'Athens':'Europe',\n",
    "'Atlantic Time (Canada)':'USA',\n",
    "'Bern':'Europe',\n",
    "'Alaska':'USA',\n",
    "'Arizona':'USA',\n",
    "'Bogota':'Latin America',\n",
    "'Mumbai':'Asia',\n",
    "'India':'Asia',\n",
    "'Berlin':'Europe',\n",
    "'Hong Kong':'Asia',\n",
    "'Seoul':'Asia',\n",
    "'Pretoria':'Africa',\n",
    "'Sydney':'Asia',\n",
    "'Muscat':'Middle East',\n",
    "'Baghdad':'Middle East',\n",
    "'Dublin':'Europe',\n",
    "'Berlin':'Europe',\n",
    "'Casablanca':'Africa',\n",
    "'Cairo':'Africa',\n",
    "'Abu Dhabi':'Middle East',\n",
    "'Chennai':'Asia',\n",
    "'Kuwait':'Middle East',\n",
    "'Kuala Lumpur':'Asia',\n",
    "'Brussels':'Europe',\n",
    "'Moscow':'Asia',\n",
    "'Central America':'Latin America',\n",
    "'Ljubljana':'Europe',\n",
    "'Singapore':'Asia',\n",
    "'Melbourne':'Asia'}\n",
    "\n",
    "\n",
    "#Removing 8 columns that are not being used\n",
    "def drop_columns(df):\n",
    "    df.drop(['Id','User Name','Location','UTC Offset','Profile Image','Profile Text Color',\n",
    "               'Profile Page Color','Profile Theme Color'],axis=1,inplace=True)\n",
    "    \n",
    "def location_fix(df):\n",
    "    '''\n",
    "    this function is to replace city with continent\n",
    "    '''\n",
    "    for i in time_zone_dict.items():\n",
    "        df['User Time Zone'] = df['User Time Zone'].replace(i[0], i[1])\n",
    "\n",
    "    top_used_loc=['USA','Europe','Latin America','Asia','Middle East','Africa']\n",
    "    df['User Time Zone'][~df['User Time Zone'].isin(top_used_loc)]='Others'\n",
    "    \n",
    "\n",
    "def preprocessing_num(df):\n",
    "    # Converting personal url to binary\n",
    "    df['Personal URL'].fillna(0,inplace=True)\n",
    "    df['Personal URL'][df['Personal URL']!=0]=1\n",
    "\n",
    "    # Converting '??' from the Location Public Visibility to enabled\n",
    "    df['Location Public Visibility']=df['Location Public Visibility'].str.lower()\n",
    "    df['Location Public Visibility']=df['Location Public Visibility'].replace('??','enabled')\n",
    "    \n",
    "    # These four languages are the most common. Other languages are converted to 'others'\n",
    "    top_used_lang=['en','es','pt','fr']\n",
    "    df['User Language'][~df['User Language'].isin(top_used_lang)]='others'\n",
    "\n",
    "    # ' ' value in Profile Category  column is converted to 'unkown'\n",
    "    df['Profile Category']=df['Profile Category'].replace(' ','unknown')\n",
    "    \n",
    "    # Here we do a log transform for four continuous valued inputs to remove the skew in the features and \n",
    "    # get feature values that resembles a normal distribution.\n",
    "\n",
    "    df['Num of Followers']= np.log10(1+df['Num of Followers'])\n",
    "    df['Num of People Following']= np.log10(1+df['Num of People Following'])\n",
    "    df['Num of Status Updates']= np.log10(1+df['Num of Status Updates'])\n",
    "    df['Num of Direct Messages']= np.log10(1+df['Num of Direct Messages'])\n",
    "\n",
    "    \n",
    "    # We do a log transform of the 'Avg Daily Profile Visit Duration in seconds' column and also impute the \n",
    "    # NaN values by the mean value of the column.\n",
    "    df['Avg Daily Profile Visit Duration in seconds']=np.log10(1+df['Avg Daily Profile Visit Duration in seconds'])\n",
    "    df['Avg Daily Profile Visit Duration in seconds'].fillna((df['Avg Daily Profile Visit Duration in seconds'].mean()), inplace=True)\n",
    "\n",
    "    # Same procedure is done for 'Avg Daily Profile Clicks' column also\n",
    "    df['Avg Daily Profile Clicks']= np.log10(1+df['Avg Daily Profile Clicks'])\n",
    "    df['Avg Daily Profile Clicks'].fillna((df['Avg Daily Profile Clicks'].mean()), inplace=True)\n",
    "\n",
    "    # We fill the NaN values in 'Profile Cover Image Status' column by 'Not set'\n",
    "    df['Profile Cover Image Status'].fillna('Not set',inplace=True)\n",
    "    \n",
    "\n",
    "\n",
    "def preprocessing_category(df):\n",
    "\n",
    "\n",
    "    # Now we convert the categorical column values from text form to numerical form to input it to the model\n",
    "    cleanup_nums = {\"Personal URL\": {\"0\":0, \"1\":1},\n",
    "                \"Profile Cover Image Status\":     {\"Not set\": 0, \"Set\": 1},\n",
    "                \"Profile Verification Status\": {\"Not verified\": 0, \"Pending\": 1, \"Verified\": 2 },\n",
    "                \"Is Profile View Size Customized?\":{\"False\":0,\"True\":1},\n",
    "                \"Location Public Visibility\":{'disabled':0,'enabled':1},\n",
    "                \"Profile Category\":{'unknown':0,'government':1,\"business\":2,'celebrity':3},\n",
    "                \"User Time Zone\":{'Others':0,'Africa':1,'Middle East':2,'Asia':3,'Latin America':4,'Europe':5,'USA':6},\n",
    "                'User Language':{'others':0,'fr':2,'pt':3,'es':4,'en':5}\n",
    "               }\n",
    "\n",
    "    # Converting the data type of the categorical columns to 'str'\n",
    "    df['Profile Cover Image Status'] = df['Profile Cover Image Status'].astype(str)\n",
    "    df['Profile Verification Status'] = df['Profile Verification Status'].astype(str)\n",
    "    df['Is Profile View Size Customized?'] =df['Is Profile View Size Customized?'].astype(str)\n",
    "    df['Location Public Visibility'] = df['Location Public Visibility'].astype(str)\n",
    "\n",
    "    df = df.replace(cleanup_nums)\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "# Creating 8 new columns from the existing features.\n",
    "def new_columns(df):\n",
    "    # Convert the time stamp column into a new column that represents the number of months \n",
    "    # the person has been on social media\n",
    "    df['Profile Creation Timestamp'] = df['Profile Creation Timestamp'].astype(str)\n",
    "    df['Profile Creation Timestamp'] =pd.to_datetime(df['Profile Creation Timestamp'])\n",
    "    df['MonthsInSocialMedia'] = ((2020- df['Profile Creation Timestamp'].dt.year) * 12 +\n",
    "    (11 - df['Profile Creation Timestamp'].dt.month))\n",
    "    \n",
    "    ### new columns\n",
    "    df['MonthsInSocialMedia'] =np.log10(1+df['MonthsInSocialMedia'])    \n",
    "        \n",
    "    df['Months follower ratio']=df['Num of Followers']/df['MonthsInSocialMedia']\n",
    "    df['Months following ratio']=df['Num of People Following']/df['MonthsInSocialMedia']\n",
    "    df['Months status ratio']=df['Num of Status Updates']/df['MonthsInSocialMedia']\n",
    "    df['Months messages ratio']=df['Num of Direct Messages']/df['MonthsInSocialMedia']\n",
    "    group_col = df[['Num of Followers', 'Num of People Following', 'Num of Status Updates', \n",
    "                    'Num of Direct Messages',\n",
    "                    'Avg Daily Profile Visit Duration in seconds', 'Avg Daily Profile Clicks']]\n",
    "    df['group_sum'] = np.sum(group_col, axis=1)\n",
    "    df['group_sum']=df['group_sum']/6\n",
    "    \n",
    "    df['Total Activity']=df['Num of Status Updates']+df['Num of Direct Messages']\n",
    "    df['Total clicks from inception']=df['Avg Daily Profile Clicks']*30*train_loc['MonthsInSocialMedia']\n",
    "\n",
    "\n",
    "# Load data\n",
    "# Use this if you are running it on kaggle notebook.\n",
    "train = pd.read_csv('../input/ift6758-a20/train.csv')\n",
    "test = pd.read_csv('../input/ift6758-a20/test.csv')\n",
    "\n",
    "tid=test['Id']\n",
    "test_id=tid.to_numpy()\n",
    "\n",
    "train_x=train.iloc[:,:24]\n",
    "train_y=train.iloc[:,23]\n",
    "\n",
    "train_loc=train_x.copy()\n",
    "\n",
    "# We dropped irrelavant and sparse columns\n",
    "drop_columns(train_loc)\n",
    "drop_columns(test)\n",
    "\n",
    "# We replaced city with continent\n",
    "location_fix(train_loc)\n",
    "location_fix(test)\n",
    "\n",
    "#data cleaning and preprocessing steps\n",
    "preprocessing_num(train_loc)\n",
    "preprocessing_num(test)\n",
    "\n",
    "train_loc=preprocessing_category(train_loc)\n",
    "test=preprocessing_category(test)\n",
    "# We add new columns\n",
    "new_columns(train_loc)\n",
    "new_columns(test)\n",
    "\n",
    "# We drop the 'Profile Creation Timestamp' because we extracted the useful information \n",
    "# from this column and stored it in 'MonthsInSocialMedia' column.\n",
    "\n",
    "train_loc.drop('Profile Creation Timestamp',axis=1,inplace=True)\n",
    "test.drop('Profile Creation Timestamp',axis=1,inplace=True)\n",
    "\n",
    "# Drop Label from the training features.\n",
    "train_loc.drop(['Num of Profile Likes'],axis=1,inplace=True)\n",
    "\n",
    "# Training features has been stored in a different variable for convenience.\n",
    "fit_x_all =train_loc.copy()\n",
    "\n",
    "# Performing log10 transform on the labels to bring it on the same scale as input features.\n",
    "fit_y_all = np.log10(1+train_y)\n",
    "\n",
    "# Building a SVR model with RBF Kernel\n",
    "svr = SVR(kernel='rbf', epsilon=0.2,C=0.75)\n",
    "\n",
    "# Building XGBRegressor\n",
    "xgboost = XGBRegressor(learning_rate=0.03,\n",
    "                       n_estimators=250,\n",
    "                       max_depth=3,\n",
    "                       seed=27,\n",
    "                       alpha=2,\n",
    "                       random_state=1)\n",
    "\n",
    "# Building Stacking regressor model with base model as xgboost and svr and the meta regressoras xgboost\n",
    "stack = StackingCVRegressor(regressors=(xgboost, svr),\n",
    "                                meta_regressor=xgboost,\n",
    "                                use_features_in_secondary=True,random_state=15)\n",
    "\n",
    "# Perform Standard Scaler on the input columns to shift the data distribution to have common scale.\n",
    "# Creating a pipeline with standard scaler and the model and then fitting it on the training data.\n",
    "stack_te = make_pipeline(StandardScaler(), stack).fit(fit_x_all, fit_y_all)\n",
    "\n",
    "# Use the model to make prediction the test data\n",
    "test_predl = stack_te.predict(test)\n",
    "\n",
    "# Performing inverse log transform(raise to 10) on the predictions \n",
    "test_pred =(10**test_predl) - 1\n",
    "# Negative predictions for the number of likes are converted to 0.\n",
    "test_pred[test_pred < 0] = 0\n",
    "\n",
    "# Rounding the predictions\n",
    "output = np.round_(test_pred)\n",
    "\n",
    "# Creating the prediction file titled 'best_stack.csv'\n",
    "sub = open('best_stack.csv','w+')\n",
    "sub.write('Id,Predicted\\n')\n",
    "for index, prediction in zip(test_id,output):\n",
    "    sub.write(str(index) + ',' + str(prediction) + '\\n')\n",
    "sub.close()\n",
    "\n",
    "# The prediction file that has been created can be submitted on Kaggle to reproduce our best score."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "papermill": {
   "duration": 38.919826,
   "end_time": "2020-12-19T20:17:15.459711",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2020-12-19T20:16:36.539885",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
